# Deep trading models implemented

<font size="2"><b>Model 1: single stock regression</b></font><div><font size="2">basic regression model: y=wx+b</font></div><div><font size="2">algorithm: gradient descent (with MSE loss)</font></div><div><font size="2">result: performs bad on test set (overfitting)</font></div>

<div><b style=""><font size="2">Model 2: multi stock regression</font></b></div><div><span style=" font-size: small;">introduce two symbols</span></div><div><span style=" font-size: small;">reasons:&nbsp;</span><span style=" font-size: small;">price can't be predicted accurately</span></div><div><font size="2" style="">invest goals changed:&nbsp;</font></div><div><font size="2" style="">1) initial goals:&nbsp;</font><span style=" font-size: small;">maximize risk adjusted return,&nbsp;</span><span style=" font-size: small;">consistency of returns over time,&nbsp;</span><span style=" font-size: small;">low market exposure by p</span><font size="2" style="">redicting the actual movement in price,&nbsp;</font><span style=" font-size: small;">buy if the the predicted price movement is positive, sell if it is negative (implied policy)&nbsp;</span></div><div><span style=" font-size: small;">2) adjusted goals: winning slightly more than losing (</span><span style=" font-size: small;">policy explicitly, 'Policy Based' deep learning)</span></div><div><span style=" font-size: small;"><br></span></div><div><div><b><font size="2">Model 3: policy training regression</font></b></div></div><div><span style=" font-size: small;">policy: chose a position (long/neutral/short) for each symbol, train the network to choose the best position&nbsp;(</span><em style=" font-size: small;">explicit</em><span style=" font-size: small;">&nbsp;and&nbsp;</span><em style=" font-size: small;">trained</em><span style=" font-size: small;">&nbsp;directly)</span></div><div><span style=" font-size: small;">summary :</span></div><div><span style=" font-size: small;">1. For each symbol,&nbsp;</span><span style=" font-size: small; font-weight: 600;">sample</span><span style=" font-size: small;">&nbsp;the probability distribution of three position buckets to get policy decision (a position long/short/neutral)</span></div><div><span style=" font-size: small;">2. multiply decision (element of {-1,0,1}) by the target value to get a daily return for the symbol</span></div><div><span style=" font-size: small;">3. add that value for all the symbols to get a full daily return</span></div><div><span style=" font-size: small;">4. select the appropriate columns from the output and combine them into a new tensor</span></div><div><span style=" font-size: small;"><br></span></div><div><p><font size="2"><b>Model 4: stochastic gradient decent</b></font></p><div><font size="2" style="">findings: using each day's return or the position on each symbol every day</font><span style=" font-size: small;">&nbsp;works better than&nbsp;</span><span style=" font-size: small;">grading it on the return over the entire dataset</span><span style=" font-size: small;">.&nbsp;</span></div><div><span style=" font-size: small;">explanation:&nbsp;</span><span style=" font-size: small;">if just take the total return over several years and its slightly positive then tell the machine to do more of that. That will do almost nothing since so many of those decisions were actually losing money</span></div><div><span style=" font-size: small;">conclusion: the problem needs to be broken down into smaller units</span></div><div><span style=" font-size: small;">solution: Stochastic Gradient Descent (for bigger project, try momentum and Adagrad, using AWS and GPU)</span></div><div><span style=" font-size: small;">advantages: able to iterate faster with smaller batches, can deal with larger dataset</span></div></div>
